# ODSOFT REPORT

This is the repository of g202, and the members are:

- Gonçalo Pinho - 1220257

- João Rocha - 1220256

- Nuno Leite - 1220271

- Tiago Lacerda - 1220285

## Pipeline Design

Initialy we started to design the pipelines (Sequential and parallel).

**Sequential pipeline**

![Alt text](pipeline-diagrams/sequential_pipeline.png?raw=true "Sequencial pipeline diagram")

Sequential pipeline, as the name says, executes the stages in a sequence.

**Parallel pipeline**

![Alt text](pipeline-diagrams/parallel_pipeline.png?raw=true "Sequencial pipeline diagram")

Parallel pipeline, can execute more than one task at the same time so that we can use hardware more efficiently and save time (or not).

## Group Tasks:

- [x] **1º Task - Repository Checkout:**

For that we used the snippet generator and picked the sample step "checkout" where we indicate the repository and branch and we can generate the code snippet that will checkout the repository.

Created the pipeline and created the stage checkout.

```groovy

node{

stage('checkout'){

checkout([$class:  'GitSCM',  branches:  [[name:  '*/master']],  extensions:  [],  userRemoteConfigs:  [[url:  'git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git']]])

}

}



```

This step, when executed for the first time, will fetch all the data from the repository. The next times will only fetch the updates.

- [x] **2º Task - Build and publish the deployment file**

On this task we've created the "build" stage where we built the application in **production mode** and generated a .war/.jar file.

For that we used the [vaadin documentation](https://vaadin.com/docs/latest/overview), and to build in production mode we need to enable it in the gradle.build file adding the following tag into the vaadin configuration:

```groovy

productionMode =  true

```

If you want to generate a war file you need to import the war plugin by adding it on the build.gradle plugins task.

```groovy

id 'war'

```

After that we can finally build the app with the following command:

`./gradlew clean build "-Pvaadin.productionMode" war`

_Yes, leave the quotation marks. Vaadin documentation have a little mistake that might take us some hours to figure it out._

Note: if you want a .war artifacts leave the `war` tag.If you prefer a .jar artifacts remove it.

After it builds, on the `build/libs` directory you can find the artifacts generated by the build.

Since there's two .war files in the same path, we thought that was better to copy one of them to the root of the repository, so we created a task to make it easier for us. The task is the following:

```groovy
tasks.register('copyArtifact', Copy) {
	from layout.buildDirectory.file("libs/flowcrmtutorial-0.0.1-SNAPSHOT.war")
	into layout.buildDirectory.dir("../")
}
```

To publish we had to add this code snippet `archiveArtifacts artifacts: 'flowcrmtutorial-0.0.1-SNAPSHOT.war', followSymlinks: false` that we indicate the artifact path and it will be published on the job front page.

- [x] **3º Task - Generate and publish javadoc**

To complete this last team goal we followed the gradle documentation [about javadoc](https://docs.gradle.org/current/dsl/org.gradle.api.tasks.javadoc.Javadoc.html) and created a new task to generate the javadoc on the build.gradle file.

```groovy

javadoc {

source = sourceSets.main.allJava

destinationDir = reporting.file("javadoc")

}

```

After that we ran `./gradlew javadoc` it will generate the javadoc on the following directory `build/reports/javadoc`.

To publish it, we need to install [HTML Publisher](https://plugins.jenkins.io/htmlpublisher/) that is a Jenkins plugin that allow us to publish HTML files on the pipeline.

After installing it, with snippet generator we selected "publishHTML" sample step and we had to provide the directory, the index page and the report title. With that, we generated the code snippet and implemented it on our Jenkinsfile resulting on the following stage:

```groovy

stage('javadoc'){

if (isUnix()){

sh './gradlew javadoc'

}else{

bat './gradlew javadoc'

}

publishHTML([allowMissing:  false,  alwaysLinkToLastBuild:  false,  keepAll:  false,  reportDir:  'build/reports/javadoc/',  reportFiles:  'index.html',  reportName:  'Javadoc',  reportTitles:  '',  useWrapperFileDirectly:  true])

}

```

After running the stage successfully on the pipeline, on the left side tab we can see a new option with the report name.

![Side tab](https://i.imgur.com/NPkp29U.png)

If we click on it we get redirected to the report!

# Individual Goals

## Integration Tests

Integration test is a type of software testing in which the different units, modules or components of a software application are tested as a combined entity.

- [x] **Integration Tests Execution**

For this stage, we created another integration test to add to the project:

```java

@Test

public  void  formShownWhenContactDeselected()  {

Grid<Contact> grid = listView.grid;

Contact emptyContact =  new  Contact();

ContactForm form = listView.form;

form.setVisible(true);

assertTrue(form.isVisible());

grid.asSingleSelect().setValue(emptyContact);

form.setVisible(false);

assertFalse(form.isVisible());

assertEquals(emptyContact.getFirstName(), form.firstName.getValue());

}

```

This test is meant to verify if the form stays invisible after pressing the button. The test starts by initializing an empty contact, after that opens a contact form and verify if it is visible. If so, the value of the empty contact is set and the form isn't visible anymore. Last, there is a comparation between the first name of the empty contact and the first name present in the form.

- [x] **Integration Tests HTML Report Generation**

To generate the HTML test report, we need to create a task of type `Test` and we named it `integrationTest`. For this, we used the [JUnit framework](https://junit.org/junit5/), and we specify that we require a HTML report, which will be generated in the directory `/build/htmlReports/junitReports/integration`. We also applied a filter that only executes tests that contains "IT" in the name.

```java

task integrationTest(type: Test)  {

useJUnitPlatform()

reports {

reports.html.required =  true

reports.html.destination =  file("/build/htmlReports/junitReports/integration")

}

filter{

includeTestsMatching "*IT"

}

}

```

- [x] **Integration Tests HTML Report Publishing**

To publish the Integration Tests HTML Report, we only need to add this stage to the JenkinsFile:

```groovy

stage('integrationReport'){

if (isUnix()) {

sh './gradlew integrationTest'

} else {

bat './gradlew integrationTest'

}

publishHTML([allowMissing:  false,  alwaysLinkToLastBuild:  false,  keepAll:  false,  reportDir:  'build/htmlReports/junitReports/integration',  reportFiles:  'index.html',  reportName:  'IntegrationTests Report',  reportTitles:  '',  useWrapperFileDirectly:  true])

}

```

This command allows Jenkins to find the report in the mentioned directory `build/htmlReports/junitReports/integration` and look for the `index.html`. Then, Jenkins generate a report named IntegrationTests Report.

- [x] **Integration Tests HTML Coverage Report Generation**

To generate the HTML coverage report, we had to create a new task named `jacocoIntegrationReport` of type `org.gradle.testing.jacoco.tasks.JacocoReport`.

First, we set the directory to get the execution data of integration tests, generated by default by the `integrationTest` task.

```groovy

task jacocoIntegrationReport(type:  org.gradle.testing.jacoco.tasks.JacocoReport) {

sourceSets sourceSets.main

getExecutionData().setFrom("/build/jacoco/integrationTest.exec")

reports {

html.enabled =  true

xml.enabled =  false

csv.enabled =  false

}

}

```

- [x] **Integration Tests HTML Coverage Report Publishing**

To publish the Integration Tests HTML Report, we only need to add this stage to the JenkinsFile:

```groovy

stage('integrationReportCoverage'){

if (isUnix()){

sh './gradlew jacocoIntegrationReport'

} else {

bat './gradlew jacocoIntegrationReport'

}

publishHTML([allowMissing:  false,  alwaysLinkToLastBuild:  false,  keepAll:  false,  reportDir:  'build/reports/jacoco/jacocoIntegrationReport/html',  reportFiles:  'index.html',  reportName:  'IntegrationTests Coverage Report',  reportTitles:  '',  useWrapperFileDirectly:  true])

}

```

This command allows Jenkins to find the report, generated by Jacoco, in the mentioned directory `build/reports/jacoco/jacocoIntegrationReport/html` and look for the `index.html`. Then, Jenkins generate a report named IntegrationTests Coverage Report.

## Unit Tests

Unit tests are the tests of each unit or an individual component of the software application.

- [x] **Unit Tests Execution**

For this stage, we created another unit test to add to the project:

```java
@Test

public  void  loginFormTesting()  {

	LoginForm  formtest  =  new  LoginForm();
	formtest.setAction("login");
	LoginEvent  form  =  new  LoginEvent(formtest,  false,  this.username,  this.password);
	assertEquals("user",  form.getUsername());
	assertEquals("userpass",  form.getPassword());

}

```

This unit test is responsible to verify if the Login Form is working correctly. First of all, creates a LoginForm and set its login action. Furthermore, was instantiated a LoginEvent with the username and password whose should be the corrects ones. Finally, the assertEquals confirmed that the username and password, passed in LoginEvent, and those expected values are equal.

- [x] **Unit Tests HTML Report Generation**

To generate the HTML test report, we need to create a task of type `Test` and we named it as `unitTest`. For this, we used the [JUnit framework](https://junit.org/junit5/), and we specify that we require a HTML report, which will be generated in the directory `/build/htmlReports/junitReports/unit`. We also applied a filter that only executes tests that contains "Test" in the name.

```java
task unitTest(type: Test) {
    useJUnitPlatform()
    reports {
        reports.html.required = true
        reports.html.destination = file("build/htmlReports/junitReports/unit")
    }
    filter{
        includeTestsMatching "*Test"
    }
}


```

- [x] **Unit Tests HTML Report Publishing**

To publish the Unit Tests HTML Report, we only need to add this stage to the JenkinsFile:

```groovy

    stage('unitReport'){
        echo 'Running Unit Tests...'
        if (isUnix()){
            sh './gradlew unitTest'
        }else{
            bat './gradlew unitTest'
        }
        echo 'Generating Unit Test HTML Report and started Publishing...'
        publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/unit', reportFiles: 'index.html', reportName: 'UnitTests Report', reportTitles: '', useWrapperFileDirectly: true])
        echo 'Published Unit Test HTML Report!'
    }

```

This command allows Jenkins to find the report in the mentioned directory `build/htmlReports/junitReports/unit` and look for the `index.html`. Then, Jenkins generate a report named UnitTests Report.

- [x] **Unit Tests HTML Coverage Report Generation**

To generate the HTML coverage report, we had to create a new task named `jacocoUnitReport` of type `org.gradle.testing.jacoco.tasks.JacocoReport`.

First, we set the directory to get the execution data of unit tests, generated by default by the `unitTest` task.

```groovy

task jacocoUnitReport(type: org.gradle.testing.jacoco.tasks.JacocoReport) {
	sourceSets sourceSets.main

    getExecutionData().setFrom("build/jacoco/unitTest.exec")

    reports {
        html.enabled = true
        xml.enabled = false
        csv.enabled = false
    }
}

```

- [x] **Unit Tests HTML Coverage Report Publishing**

To publish the Unit Tests HTML Report, we only need to add this stage to the JenkinsFile:

```groovy

stage('unitReportCoverage'){
    echo 'Generating Unit Test Coverage Report...'
    if (isUnix()){
        sh './gradlew jacocoUnitReport'
    }else{
        bat './gradlew jacocoUnitReport'
    }
    echo 'Generated Unit Test Coverage Report and started Publishing...'
    publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoUnitReport/html', reportFiles: 'index.html', reportName: 'UnitTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
    echo 'Published Unit Test Coverage Report!'
    }

```

This command allows Jenkins to find the report, generated by Jacoco, in the mentioned directory `build/reports/jacoco/jacocoUnitReport/html` and look for the `index.html`. Then, Jenkins generate a report named UnitTests Coverage Report.

## Mutation Tests

Mutation testing is a form of white box testing in which testers change specific components of an application's source code to ensure a software test suite will be able to detect the changes. Changes introduced to the software are intended to cause errors in the program.

In this work it was proposed that we run mutation tests to our project, to which we would have to 'Generate the Mutation Tests HTML Coverage Report' and 'Publish the Mutation Tests
HTML Coverage Report on Jenkins'.

- [x] **Mutation Tests Execution**

For this stage, we created and executed mutation test to add to the project our path where it will be our HTML report adding the following plugin to the 'plugins':

id 'info.solidsoft.pitest' version '1.7.4'

In our task with the name of 'pitest' we need to contain the version of 'junit plugin' and a command called 'timestampedReports' for our file don't come with datatime format.

````groovy

pitest  {

	junit5PluginVersion = '0.15'
	timestampedReports = false

}

- [x] **Mutation Tests HTML Coverage Report Publishing**

To publish the Mutation Tests HTML Report, we only need to add this stage to the JenkinsFile:

```groovy

stage('mutationReportCoverage'){
        echo 'Generating Mutation Test Coverage Report...'
        if (isUnix()){
            sh './gradlew pitest'
        }else{
            bat './gradlew pitest'
        }
        echo 'Generated Mutation Test Coverage Report and started Publishing...'
        publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/pitest', reportFiles: 'index.html', reportName: 'Mutation Tests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
        echo 'Published Mutation Test Coverage Report!'
}

````

This command allows Jenkins to find the report, in the mentioned directory `build/reports/pitest` and look for the `index.html`. Then, Jenkins generate a report named Mutation Tests Coverage Report.

## Pipeline Stage(5/5)

1220257 - Gonçalo Pinho was responsible for this pipeline stages.

- [x] **1º Staging Deployment**
      For this stage we thought about two possible options:
- Simulate the production environment on a docker container;
- Create a instance on a cloud service and prepare it like it is the production environment.

We picked the creation of the instance on a cloud service (Amazon Web Sevices), created a EC2 Instance with Ubuntu 22.04 and Apache Tomcat version 9.

For this stage we based ourselves on the following tutorials:

- To prepare the environment we followed [this guide](https://medium.com/@hasnat.saeed/install-tomcat-9-on-ubuntu-18-04-605ca963ffcc).
- To deploy the .war file from Jenkins to the server we followed [this video tutorial](https://www.youtube.com/watch?v=YbaPlDpV184).

Summing up both of the guides, we installed:

- [Java 17](https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html)
- [Apache Tomcat Version](https://tomcat.apache.org)

The key point of make things possible to deploy the project from Jenkins to the EC2 instance was configuring AWS EC2 security group and creating a tomcat user that is able to deploy by script.

By default the Tomcat port is 8080, and we are keeping it but, its important to know it to configure the EC2 Instance security group.

After configuring the security group we can try to access the tomcat with your ec2 Public IPv4 DNS:8080, for example: `http://ec2-3-85-104-7.compute-1.amazonaws.com:8080/`

After that you should create a tomcat user with `admin-script` `manager-script` `admin-gui` `manager-script`

Your should edit the file on this directory `tomcat/conf/tomcat-users.xml`.
And add this and edit the username and password for something of your choice:

```xml
<role rolename="admin-gui"/>
<role rolename="admin-script"/>
<role rolename="manager-gui"/>
<role rolename="manager-script"/>
<user username="admin" password="123admin456" roles="admin-gui,manager-gui,admin-script,manager-script"/>
```

And now you can deploy manually or via script, on this case with Jenkins.

Before the deploy we recommend you to encrease the max upload file since the .war file that is generated by the project is bigger than the default value (50MB). For that you should go to the following directory: `/tomcat/webapps/manager/WEB-INF/web.xml`

And change the max file size and request size for this:

```xml
<max-file-size>104857600</max-file-size>
<max-request-size>104857600</max-request-size>
```

After that we can jump to the Jenkins configuration of the credentials.

In there we want to store the tomcat username and password and the most important part is defining a id for a dynamic approach when using the jenkinsfile in multiple computers.

To deploy we used the plugin [Deploy to Container].(https://plugins.jenkins.io/deploy/)

One more time we use the jenkins pipeline syntax to generate your code, we select the sample step `deploy:Deploy war/ear to container` and in there we indicate:

- The war file path;
- The context path (this is the "/path" that you will put in front of your URL)
- The container you are using (this case Tomcat 9)
- The credentials (that we configured before)
- The tomcat url (`http://ec2-3-85-104-7.compute-1.amazonaws.com:8080/`)

And generate the snippet:

```groovy
deploy adapters: [tomcat9(credentialsId: "odsoft", path: "", url: "http://ec2-3-85-104-7.compute-1.amazonaws.com:8080/")], contextPath: "crm", war: "flowcrmtutorial-0.0.1-SNAPSHOT.war"
```

Well, at this point we had the everything ready, ran the job in jenkins and then occurred an error deploying the application to the Tomcat.

Then we reached to the conclusion that to deploy the .war file we had to do some more things, so we searched in springboot docs and found [this documentation](https://docs.spring.io/spring-boot/docs/2.1.1.RELEASE/reference/html/howto-traditional-deployment.html) about deploying.

Long story short, we had to extend our Application with the `SpringBootServletInitializer` abstract classs and override the following method:

```java
@Override
protected  SpringApplicationBuilder  configure(SpringApplicationBuilder  application) {
	return  application.sources(Application.class);
}
```

After that, the deploy was successful and we could open our beautiful application through the ec2 tomcat link + /crm, something like this: `http://ec2-3-85-104-7.compute-1.amazonaws.com:8080/crm `!

- [x] **2º - System Test**
      For the system test we will perform a automatic smoke test that will be checking if the base URL of the application is responsive after staging the deployment.

For that we used the command line tool curl, that stands for client url, that is normally used for transfering data using various network protocols.

We could have just print all the headers of the server response and that contains the http reponse. But we wanted to go a bit further and only print the http code.
And depending on the operating system that the job is running, we had to make some changes.
**For Linux**
For linux we used the following command:
`curl -s -o /dev/null -w '%{http_code}' $url/crm`
Where `$url` is the variable that holds the url that you want to get the http code.

**For Windows**
For windows we used the following command
`curl -s -o ./response -w '%%{http_code}' $url/crm"`

Knowing the commands we just created a way to store the result and create a condition to verify is the response was positive and we could proceed or negative and we throw an error and abort the build.

```groovy
stage('systemTest'){
	echo "Initiating Smoke Test"
	if (isUnix()){
		httpCode = sh( script: "curl -s -o /dev/null -w '%{http_code}' $url/crm", returnStdout: true ).trim()
	}else{
		httpCode = bat( script: "curl -s -o ./response -w '%%{http_code}' $url/crm", returnStdout: true).trim()
		httpCode = httpCode.readLines().drop(1).join(" ")//windows returns full command plus the response, but the response is at a new line so we can drop the first line and remove spaces and we get only the http code
	}
//checking if the http code was ok(200) or found(302)
	if (httpCode == "200" || httpCode == "302"){
		echo 'The application is responding!'
	}else{
		currentBuild.result = 'ABORTED'
		error('The application is not responding...')
	}
}
```

- [x] **3º - UI Manual Acceptance Test**

For this stage we used [Sendinblue](https://www.sendinblue.com), but you can use the SMTP server of your choice, but you can follow the steps that we did.

First of all, we need a Jenkins plugin so that we can send emails through the Jenkinsfile, and for that we used [Email Extension](https://plugins.jenkins.io/email-ext/).

After installing the plugin, to configure it, we click on `Manage Jenkins > Configure System` and scroll down to `Extended E-mail Notification`. We indicate the SMTP server and the SMTP port and then click on `Advanced` and create credentials with the SMTP credentials that you have.

Note: If you authenticate via SMTP Authentication do not select any of the below checkboxes!

Finishing the configuration part, we opened the snippet generator with the sample step `emailtext: Extended Email`.
We just used the field to indicate the reciever and the message and generated the following script:

```groovy
emailext body: "Greetings developer,\n I'm here to tell you that the application is up and running! Now you should manually test it to confirm if it meets your standarts\n The link is: http://ec2-3-85-104-7.compute-1.amazonaws.com:8080/crm\n After that please proceed to manually confirm that you want to proceed or abort with the following link: http://localhost:8081/job/${env.JOB_NAME}/${env.BUILD_NUMBER}/console"  \n This is an automated message from your Jenkins job.", subject: "Job Manual Test of Build#${env.BUILD_NUMBER}", to: "1220257@isep.ipp.pt"
```

When you run the job with the shown script on the pipeline, if everything was correctly configured, you will get an email!

For the user input to Proceed or Abort the build, we used Jenkins documentation, and after understanding that Jenkins has a command that allows to create user input on the console, lead to the following script:

```groovy
	userInput = input(id: 'userInput',
	message: 'Have you manually tested the application?',
	parameters: [[$class:'ChoiceParameterDefinition', choices: "Yes\nNo", name: 'Answer']])
```

This input, will pause the build until the user inputs the parameters and clicks on the button "Proceed" or "Abort".

In case you click "Proceed", it will continue the pipeline, otherwise will abort it.

- [x] **Continuous Integration Feedback**

On this last task, we had to tag the last commit that was running on the job with the build number and the build result.

For that we pushed a tag to the repository that contains the build number and the result of the build.

Since on this first part of the assignment we are restricted to the use of scripted pipelines we can't use the [post build action](https://www.jenkins.io/doc/book/pipeline/syntax/#post) via code, forcing us to find another solution.

After some research the best solution was wrapping all the stages with a try/catch/finally statement and on the finally push the tag to the repository.

Resulting on the following stage on the Jenkinsfile:

```groovy
try{
//code
}catch(error){
	echo "Something went wrong..."
	throw error
}finally{
	echo 'Continuous Integration Feedback'
	if (isUnix()) {
		sh "git tag -a Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
		sh "git push git@bitbucket.org:goncalo-pinho/vaadin-crm-baseline-students-1220257.git --tags"
	}else{
		bat "git tag -a Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
		bat "git push git@bitbucket.org:goncalo-pinho/vaadin-crm-baseline-students-1220257.git --tags"
	}
}
```

That way the tag will be pushed independently of the pipeline result and last commit of the repository get associated with a tag with the following aspect #BUILD_NUMBER-BUILD_RESULT.

## Pipelines

**Recapping all stages:**

- **checkout** - In the first execution, this stage will get the git
  repository. Other executions will only get the updates from that
  repository.
- **build** - builds the project and handles the artifacts, generating and
  publishing them.
- **unitReports**- runs all the unit tests and generates a junit report,
  publishing in the pipeline.
  **unitReportCoverage** - generates and publishes the unit test coverage report.
- **integrationReport** - runs the integration tests, generates junit
  report and publishes it.
- **integrationReportCoverage** - generates integration coverage report and
  publishes it.
- **mutationReportCoverage** - run mutation tests, generates coverage
  report and publishes it.
- **javadoc** - generates javadoc and publishes it staging - publishes the
  project on a staging environment.
- **systemTest** - Runs a smoke test.
- **manualTest** - Sends an email to the user, notifying about the success
  of the deployment and asking for a manual test for the application  
  and requires a manual confirmation on the job console to continue the
  build.

Inside a finally statement there's the continuous integration feedback that tags the repository with the build number and the build result.

- [x] **Sequential pipeline analysis**
      Before we were building the pipeline as we developed the project requirements, now we stopped to think about the sequence of the stages that made more sense for us. Resulting on the following pipeline code:

```groovy
//VARIABLES
def  url = "http://ec2-18-205-25-143.compute-1.amazonaws.com:8080"
def  job_console = "http://localhost:8081/job/${env.JOB_NAME}/${env.BUILD_NUMBER}/console"

node{
try{
stage('checkout'){
	checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git']]])
}

stage('build'){
	if (isUnix()){
		sh './gradlew clean build "-Pvaadin.productionMode" war'
		sh "./gradlew copyArtifact"
	}else{
		bat './gradlew clean build "-Pvaadin.productionMode" war'
		bat "./gradlew copyArtifact"
	}
	archiveArtifacts artifacts: 'flowcrmtutorial-0.0.1-SNAPSHOT.war', followSymlinks: false

}


stage('unitReport'){
	echo 'Running Unit Tests...'
	if (isUnix()){
		sh './gradlew unitTest'
	}else{
		bat './gradlew unitTest'
	}
	echo 'Generating Unit Test HTML Report and started Publishing...'

	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/unit', reportFiles: 'index.html', reportName: 'UnitTests Report', reportTitles: '', useWrapperFileDirectly: true])

echo 'Published Unit Test HTML Report!'

}

stage('unitReportCoverage'){
	echo 'Generating Unit Test Coverage Report...'
	if (isUnix()){
		sh './gradlew jacocoUnitReport'
	}else{
		bat './gradlew jacocoUnitReport'
	}
	echo 'Generated Unit Test Coverage Report and started Publishing...'
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoUnitReport/html', reportFiles: 'index.html', reportName: 'UnitTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
	echo 'Published Unit Test Coverage Report!'

}

stage('integrationReport'){
	echo 'Running Integration Tests...'
	if (isUnix()){
		sh './gradlew integrationTest'
	}else{
		bat './gradlew integrationTest'
}
	echo 'Generating Integration Test HTML Report and started Publishing...'
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/integration', reportFiles: 'index.html', reportName: 'IntegrationTests Report', reportTitles: '', useWrapperFileDirectly: true])
	echo 'Published Integration Test HTML Report!'
}

stage('integrationReportCoverage'){
	echo 'Generating Integration Test Coverage Report...'
	if (isUnix()){
		sh './gradlew jacocoIntegrationReport'
	}else{
		bat './gradlew jacocoIntegrationReport'
	}
	echo 'Generated Integration Test Coverage Report and started Publishing...'
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoIntegrationReport/html', reportFiles: 'index.html', reportName: 'IntegrationTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
	echo 'Published Integration Test Coverage Report!'
}

stage('mutationReportCoverage'){
	echo 'Generating Mutation Test Coverage Report...'
	if (isUnix()){
		sh './gradlew pitest'
	}else{
		bat './gradlew pitest'
	}
	echo 'Generated Mutation Test Coverage Report and started Publishing...'
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/pitest', reportFiles: 'index.html', reportName: 'Mutation Tests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
	echo 'Published Mutation Test Coverage Report!'
}

stage('javadoc'){
	if (isUnix()){
		sh './gradlew javadoc'
	}else{
		bat './gradlew javadoc'
	}
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/javadoc/', reportFiles: 'index.html', reportName: 'Javadoc', reportTitles: '', useWrapperFileDirectly: true])
}

stage('staging'){
	echo "Starting Staging Fase..."
	echo "Deploying to environment..."
	deploy adapters: [tomcat9(credentialsId: "odsoft", path: "", url: "$url")], contextPath: "crm", war: "flowcrmtutorial-0.0.1-SNAPSHOT.war"
	echo "Stage deployed!"

}

stage('systemTest'){
	echo "Initiating Smoke Test"
	if (isUnix()){
		httpCode = sh( script: "curl -s -o /dev/null -w '%{http_code}' $url/crm", returnStdout: true ).trim()
	}else{
		httpCode = bat( script: "curl -s -o ./response -w %%{http_code} $url/crm", returnStdout: true).trim()
		httpCode = httpCode.readLines().drop(1).join(" ")//windows returns full command plus the response, but the response is at a new line so we can drop the first line and remove spaces and we get only the http code
	}

	//checking if the http code was ok(200) or found(302)
	if (httpCode == "200" || httpCode == "302"){
		echo 'The application is responding!'
	}else{
		currentBuild.result = 'ABORTED'
		error('The application is not responding...')
	}
}

stage('manualTest'){
echo 'Sending email...'
emailext body: "Greetings developer,\n I'm here to tell you that the application is up and running! Now you should manually test it to confirm if it meets your standarts\n The link is: $url/crm\n After that please proceed to manually confirm that you want to proceed or abort with the following link: $job_console  \n This is an automated message from your Jenkins job.", subject: "Job Manual Test of Build#${env.BUILD_NUMBER}", to: "1220257@isep.ipp.pt"
echo 'Waiting for manual confirmation...'
userInput = input(id: 'userInput',
message: 'Have you manually tested the application?',
parameters: [
[$class:'ChoiceParameterDefinition', choices: "Yes\nNo", name: 'Answer']
])
}
}catch(error){
	echo "Something went wrong..."
	throw error
}finally{
	echo 'Continuous Integration Feedback'
if (isUnix()) {
	sh "git tag -a Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
	sh "git push git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git --tags"
}else{
	bat "git tag -a Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
	bat "git push git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git --tags"
}
}
}
```

For a clear view of how the sequential pipeline was designed we created the following diagram:
![Sequential pipeline diagram](https://i.imgur.com/AQ9A7C5.jpg)
The logic of this pipeline implementation was in running in a sequence the stages that depend on eachother.
As we can see we ran the unit tests, then we generated and publish the coverage report, we ran the integration tests and generated the coverage report, and so forth.

Having the pipeline created we made a three runs so that we could make some analysis.
![Sequential pipeline runs](https://i.imgur.com/cS6EdL0.png)
Analysign the run as a whole we can see that:

- Run A took 1 minute and 39 seconds (99s);
- Run B took 1 minute and 40 seconds (100s);
- Run C took 1 minute and 47 seconds (107s);

The average time to run the sequential pipeline is approximately 1 minute and 42 seconds.

Analysis stage by stage:

- The **checkout** stage the average run time is 1 seconds;
- The **build** stage the average run time is 20 seconds;
- The **unitReport** stage average run time is 4 seconds;
- The **unitReportCoverage** average run time is 2 seconds;
- The **integrationReport** average runtime is 7 seconds;
- The **integrationReportCoverage** average run time is 7 seconds;
- The **mutationReportCoverage** average run time is 37 seconds;
- The **javadoc** average run time is 3 seconds;
- The **staging** average run time is 24 seconds;
- The **systemTest** average run time is 406 milliseconds;
- The **manualTest** average run time is 849 milliseconds.

The task that took more time to run was the mutationReportCoverage.

- [x] **Parallel pipeline analysis**
      We followed the same logic as the sequential pipeline, we stoped for a while and thought about the stages and their dependencies and implemented the pipeline that we can see on the following code:

```groovy
//VARIABLES
def  url = "http://ec2-18-205-25-143.compute-1.amazonaws.com:8080"
def  job_console = "http://localhost:8081/job/${env.JOB_NAME}/${env.BUILD_NUMBER}/console"
node{
try{
stage('checkout'){
	checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git']]])
}

stage('build'){
	if (isUnix()){
		sh './gradlew clean build "-Pvaadin.productionMode" war'
		sh "./gradlew copyArtifact"
	}else{
		bat './gradlew clean build "-Pvaadin.productionMode" war'
		bat "./gradlew copyArtifact"
	}
	archiveArtifacts artifacts: 'flowcrmtutorial-0.0.1-SNAPSHOT.war', followSymlinks: false
}

stage("unitReport") {
	echo 'Running Unit Tests...'
	if (isUnix()){
		sh './gradlew unitTest'
	}else{
		bat './gradlew unitTest'
	}
	echo 'Generating Unit Test HTML Report and started Publishing...'
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/unit', reportFiles: 'index.html', reportName: 'UnitTests Report', reportTitles: '', useWrapperFileDirectly: true])
	echo 'Published Unit Test HTML Report!'
}
parallel(
"mutationReportCoverage": {
	stage("mutationReportCoverage") {
		echo 'Generating Mutation Test Coverage Report...'
		if (isUnix()){
			sh './gradlew pitest'
		}else{
			bat './gradlew pitest'
		}
		echo 'Generated Mutation Test Coverage Report and started Publishing...'
		publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/pitest', reportFiles: 'index.html', reportName: 'Mutation Tests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
		echo 'Published Mutation Test Coverage Report!'
	}
},
"unitReportCoverage": {
	stage("unitReportCoverage") {
		echo 'Generating Unit Test Coverage Report...'
		if (isUnix()){
			sh './gradlew jacocoUnitReport'
		}else{
			bat './gradlew jacocoUnitReport'
		}
		echo 'Generated Unit Test Coverage Report and started Publishing...'
		publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoUnitReport/html', reportFiles: 'index.html', reportName: 'UnitTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
		echo 'Published Unit Test Coverage Report!'
	}
},

"integrationReport": {
	stage('integrationReport'){
		echo 'Running Integration Tests...'
		if (isUnix()){
			sh './gradlew integrationTest'
		}else{
			bat './gradlew integrationTest'
		}
		echo 'Generating Integration Test HTML Report and started Publishing...'
		publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/integration', reportFiles: 'index.html', reportName: 'IntegrationTests Report', reportTitles: '', useWrapperFileDirectly: true])
		echo 'Published Integration Test HTML Report!'
	}
})

parallel(
"integrationReportCoverage": {
	stage("integrationReportCoverage") {
		echo 'Generating Integration Test Coverage Report...'
		if (isUnix()){
			sh './gradlew jacocoIntegrationReport'
		}else{
			bat './gradlew jacocoIntegrationReport'
		}
		echo 'Generated Integration Test Coverage Report and started Publishing...'
		publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoIntegrationReport/html', reportFiles: 'index.html', reportName: 'IntegrationTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
		echo 'Published Integration Test Coverage Report!'
	}
},

"javadoc":{
	stage('javadoc'){
	if (isUnix()){
		sh './gradlew javadoc'
	}else{
		bat './gradlew javadoc'
	}
	publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/javadoc/', reportFiles: 'index.html', reportName: 'Javadoc', reportTitles: '', useWrapperFileDirectly: true])
	}
},
"staging":{
	stage('staging'){
		echo "Starting Staging Fase..."
		echo "Deploying to environment..."
		deploy adapters: [tomcat9(credentialsId: "odsoft", path: "", url: "$url")], contextPath: "crm", war: "flowcrmtutorial-0.0.1-SNAPSHOT.war"
		echo "Stage deployed!"
	}
})

stage('systemTest'){
	echo "Initiating Smoke Test"
	if (isUnix()){
		httpCode = sh( script: "curl -s -o /dev/null -w '%{http_code}' $url/crm", returnStdout: true ).trim()
	}else{
		httpCode = bat( script: "curl -s -o ./response -w %%{http_code} $url/crm", returnStdout: true).trim()
		httpCode = httpCode.readLines().drop(1).join(" ")//windows returns full command plus the response, but the response is at a new line so we can drop the first line and remove spaces and we get only the http code
	}
	//checking if the http code was ok(200) or found(302)
	if (httpCode == "200" || httpCode == "302"){
		echo 'The application is responding!'
	}else{
		currentBuild.result = 'ABORTED'
		error('The application is not responding...')
	}
}

stage('manualTest'){
	echo 'Sending email...'
	emailext body: "Greetings developer,\n I'm here to tell you that the application is up and running! Now you should manually test it to confirm if it meets your standarts\n The link is: $url/crm\n After that please proceed to manually confirm that you want to proceed or abort with the following link: $job_console  \n This is an automated message from your Jenkins job.", subject: "Job Manual Test of Build#${env.BUILD_NUMBER}", to: "1220257@isep.ipp.pt"
	echo 'Waiting for manual confirmation...'
	userInput = input(id: 'userInput',
	message: 'Have you manually tested the application?',
	parameters: [
	[$class:'ChoiceParameterDefinition', choices: "Yes\nNo", name: 'Answer']
	])
}
}catch(error){
	echo "Something went wrong..."
	throw error
}finally{
	echo 'Continuous Integration Feedback'
	if (isUnix()) {
		sh "git tag -a Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
		sh "git push git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git --tags"
	}else{
		bat "git tag -a Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
		bat "git push git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git --tags"
	}
}
}
```

For a clear view of the parallel pipeline implementation we made the following diagram:
![Parallel pipeline design](https://i.imgur.com/vnmb91U.jpg)

We choose the stage mutationReportCoverage, unitReportCoverage and integrationReport to run in parallel because they have no dependencies between them and we thought we could "save" time by starting to run these three stages at the same time. As we can see after the first parallel we have another, because the last parallel generated stuff that was needed to run these other tasks so we stoped the parallel there to make sure we have the stuff that is needed for the other stages.
The other parallel, runs the integrationReportCoverage, javadoc and staging stages since they have no dependencies between them and we thought again that we could save time.

On the following image we can see three runs of the parallel pipeline.
![Parallel pipeline runs](https://i.imgur.com/vWmqZy6.png)

Full build analysis:

- Run A took 145 seconds (2 minutes and 25 seconds)
- Run B took 143 seconds (2 minutes and 23 seconds)
- Run C took 131 seconds (2 minutes and 11 seconds)

Average full build time is 140 seconds (2 minutes and 20 seconds)

Stage analysis:

- The **checkout** stage average run time is 1 second;
- The **build** stage average run time is 21 seconds;
- The **unitReport** stage average run time is 4 seconds;
- The **mutationReportCoverage** stage average run time is 48 seconds;
- The **unitReportCoverage** stage average run time is 11 seconds;
- The **integrationReport** stage average run time is 4 seconds;
- The **integrationReportCoverage** stage average run is 4 seconds;
- The **javadoc** stage average run time is 5 seconds;
- The **staging** stage average run time is 26 seconds;
- The **systemTest** stage average run time is 401 milliseconds
- The **manualTest** stage average run time is 799 milliseconds

The mutationReportCoverage was, again, the task that took more time to run.

## Pipeline analysis comparison

Comparing the stages run time we can conclude:
**Both** pipelines average run time are equal in the following stages:

- checkout
- unitReport

The **sequential** pipeline is faster on the following stages:

- build
- unitReportCoverage
- mutationReportCovarage
- javadoc
- staging

The **parallel** pipeline is faster on the following stages:

- unitReportCoverage
- systemTest
- ManualTest

Comparing the average full build time, the sequential pipeline is better by almost 40 seconds.

# Second Assignment

This second assignment started a 17 Nov and ends at 22 Dec.

## 2.1 Base pipeline and persistence

This point was developed by all team members.

We choosed to use Github flow as our branch model since its a simple method to work with branches. Github flow uses just two branches, the master and the feature, in this last one will have all commits and then we open PR to the master branch, as it is shown in the image below:

![enter image description here](https://user-images.githubusercontent.com/6351798/48032310-63842400-e114-11e8-8db0-06dc0504dcb5.png)

To trigger the pipeline we thought about using bitbucket webhook, so when the master branch receives a new commit will trigger the pipeline. To achieve this, it's necessary to go to the bitbucket repository settings and add the webhook refering the jenkins URL + /bitbucket-hook/. For jenkins to be able to receive this triggers it's necessary to have the [Bitbucket](https://plugins.jenkins.io/bitbucket/) plugin and enable the trigger on the job.

Atfer we evaluate the project and discuss each other, we made a prevision of the pipeline:
![Alt text](pipeline-diagrams/pipelineA2.png?raw=true "Pipeline Prevision Assignment2")

We opted to use the parallel pipeline because after the presentation of the first assignment we found out we could have a better performance by increasing the number of executors.
The first group of parallel stages is composed by the following stages: intializing staging environment, unit tests, check and mutation tests. The first stage doesn't wait for the server to initialize, so we thought about having other stages in the same group.
If this group is done without errors, the pipeline can continue and will do the smoke test followed by the manual acceptence test. Afterwards, the second group of parallel stages is composed by IT tests and End2End tests, finally the third one is composed with Jmeter and Cucumber stages. This happens because the group computers are not good enough to have multiple instances of web browsers.
Furthermore, the last group of parallel it's similiar as the first one, because the first stage doesn't need to wait for the production server to initialize, so we followed the same logic as before and added other stages to the group.
Finally, it's performed the last stage of the pipeline, which includes the smoke tests in the production server and the generation of a tag even if the pipeline is built or not.

We chose PostgreSQL as our relational database, because it was a group preference.

In the first assignment we only could use scripted pipeline, but in this one we had to use the declarative one, so the first step of the second assignment was to convert the previous one into a declarative pipeline.

The first staging environment we had was AWS EC2 instance, due to high costs we choosed to use docker containers.

```groovy
def version = "0.0.1"
def url = "http://localhost:8082"
def job_console = "http://localhost:8085/job/${env.JOB_NAME}/${env.BUILD_NUMBER}/console"
pipeline{
    agent any;
    options{
        timestamps()
        parallelsAlwaysFailFast() //When one of the stages of the parallel fails, everything fails.
    }
    environment {
        DB_HOST='localhost'
        DB_PORT='5433'
    }
    stages{
        stage("Checkout"){
            steps{
                script{
                    try{
                        checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git']]])
                    }catch (error){
                        currentBuild.result = 'FAILURE'
                        throw error
                    }
                }
            }
        }

        stage("Build"){
            steps{
                script{
                    try{
                        if (isUnix()){
                            //building and generating artifacts
                            sh "./gradlew clean build '-Pvaadin.productionMode' -x test war"
                            sh './gradlew renameDeployFile'
                            sh 'docker build -t odsoft-image .'

                        }else{
                            //building and generating artifacts
                            bat "./gradlew clean build -Pvaadin.productionMode -x test war"
                            bat './gradlew renameDeployFile'
                            bat "docker build -t odsoft-image ."

                        }
                        archiveArtifacts artifacts: 'build/libs/flowcrmtutorial-0.0.1-SNAPSHOT.war', followSymlinks: false
                    }catch (error){
                        currentBuild.result = 'FAILURE'
                        throw error
                    }
                }
            }
        }
        stage("Parallel 1"){
            parallel{
            //START OF PARALLEL 1
                stage("InitializingStagingEnv"){ //put in parallel with mutation and unit tests
                    steps{
                        script{
                            try{
                                if (isUnix()){
                                    sh 'docker-compose -f docker-compose-staging.yml up -d'
                                }else{
                                    bat 'docker-compose -f docker-compose-staging.yml up -d'
                                }
                            }catch(error){
                                currentBuild.result = 'FAILURE'
                                throw error
                            }
                        }
                    }
                }
                stage("unitTest"){
                    steps{
                        script{
                            try{
                                if (isUnix()){
                                    sh './gradlew unitTest'
                                    sh './gradlew jacocoUnitReport'
                                }else{
                                    bat './gradlew unitTest'
                                    bat './gradlew jacocoUnitReport'
                                }
                                publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/unit', reportFiles: 'index.html', reportName: 'UnitTests Report', reportTitles: '', useWrapperFileDirectly: true])
                                publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoUnitReport/html', reportFiles: 'index.html', reportName: 'UnitTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
                            }catch(error){
                                currentBuild.result = 'FAILURE'
                                throw error
                            }
                        }
                    }
                }
                stage('mutationTest'){
                    steps{
                        script{
                            try{
                                if (isUnix()){
                                    sh './gradlew pitest'
                                }else{
                                    bat './gradlew pitest'
                                }
                                publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/pitest', reportFiles: 'index.html', reportName: 'Mutation Tests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
                            }catch(error){
                                currentBuild.result = 'FAILURE'
                                throw error
                            }
                        }
                    }
                }
            //END OF PARRALEL 1
            }
        }
        stage('SmoketestStaging'){
            steps{
                script{
                    try{
                        if (isUnix()){
                            httpCode = sh( script: "curl -s -o /dev/null -w '%{http_code}' $url/login", returnStdout: true ).trim()
                            echo httpCode
                        }else{
                            httpCode = bat( script: "curl -s -o ./response -w %%{http_code} $url/login", returnStdout: true).trim()
                            httpCode = httpCode.readLines().drop(1).join(" ")//windows returns full command plus the response, but the response is at a new line so we can drop the first line and remove spaces and we get only the http code
                        }
                                //checking if the http code was ok(200) or found(302)
                        if (httpCode == "200" || httpCode == "302"){
                            echo 'The application is responding!'
                        }else{
                            currentBuild.result = 'FAILURE'
                            error('The application is not responding...')
                        }
                    }catch(error){
                        currentBuild.result = 'FAILURE'
                        throw error
                    }
                }
            }
        }

        stage('ManualAcceptanceTest'){
        //TODO change this stage to the staging env this doesnt make sense to be after prod
            steps{
                script{
                    try{
                        emailext body: "Greetings developer,\n I'm here to tell you that the application is up and running! Now you should manually test it to confirm if it meets your standarts\n The link is: $url/login\n After that please proceed to manually confirm that you want to proceed or abort with the following link: $job_console \n This is an automated message from your Jenkins job.", subject: "Job Manual Test of Build#${env.BUILD_NUMBER}", to: "1220257@isep.ipp.pt"
                        userInput = input(id: 'userInput',
                                message: 'Have you manually tested the application?',
                                parameters: [
                                    [$class:'ChoiceParameterDefinition', choices: "Yes\nNo", name: 'Answer']
                                        ]
                        )
                    }catch(error){
                        currentBuild.result = 'FAILURE'
                        throw error
                    }
                }
            }
        }

        stage("Parallel 2"){
            parallel{
                //START OF PARALLEL 2
                stage("integrationTest"){
                    steps{
                        script{
                            try{
                                if (isUnix()){
                                    sh './gradlew integrationTest'
                                    sh './gradlew jacocoIntegrationReport'
                                }else{
                                    bat './gradlew integrationTest'
                                    bat './gradlew jacocoIntegrationReport'
                                }
                                publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/htmlReports/junitReports/integration', reportFiles: 'index.html', reportName: 'IntegrationTests Report', reportTitles: '', useWrapperFileDirectly: true])
                                publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/jacoco/jacocoIntegrationReport/html', reportFiles: 'index.html', reportName: 'IntegrationTests Coverage Report', reportTitles: '', useWrapperFileDirectly: true])
                            }catch (error){
                                currentBuild.result = 'FAILURE'
                                throw error
                            }
                        }
                    }
                }
                //END OF PARALLEL 2
            }
        }
        stage("Javadoc"){
            steps{
                script{
                    try{
                        if (isUnix()){
                            sh "./gradlew javadoc"
                        }else{
                            bat "./gradlew javadoc"
                        }
                        publishHTML([allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: 'build/reports/javadoc/', reportFiles: 'index.html', reportName: 'Javadoc', reportTitles: '', useWrapperFileDirectly: true])
                    }catch (error){
                        currentBuild.result = 'FAILURE'
                        throw error
                    }
                }
            }
        }
    }
    post {
        always {
            script{
            if (isUnix()) {
                sh "git tag -a Version#$version-Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
                sh "git push git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git --tags"
            }else{
                bat "git tag -a Version#$version-Build#${env.BUILD_NUMBER}-${currentBuild.currentResult} -m \"Tag generated in jenkins job\""
                bat "git push git@bitbucket.org:mei-isep/odsoft-22-23-ncf-g202.git --tags"
            }
            }
        }
    }
}

```

This was the result of the conversion of the previous scripted pipeline to declarative pipeline.

From now, the elements of the group will develop their tasks.

## **2.2 Documentation and database **

The student, Gonçalo Pinho-1220257 was the one in charge of the documentation and database.

**Database persistence:**
As we said earlier on this report, we are going to use PostgreSQL.
We used the [official postgres image](https://hub.docker.com/_/postgres) from the docker hub. To make sure we persist data we needed to use docker volumes, that is a file system mounted on Docker container to preserve data generated by the running container. There's two type of volumes:

- **Named volumes:** are docker volumes that are created and managed by docker itself, and have a specific name that you can use to reference them.
- **Anonymous volumes:** are docker volumes that are created automatically when we start a new container when u specify that you want a volume. These volumes do not have a specific name, and are attached to the specific container that they are created for.

In this case we used named volumes, because we thought that it would better fit for our work in case in the future we need to clone the volume from the production environment and use it in the Staging one.

To run the postgres image in a container with persistence data we use the following command `docker run --name database -e POSTGRES_PASSWORD=12345 -e POSTGRES_USER=postgres -e POSTGRES_DB=odsoft -v odsoft_data:/var/lib/postgresql/data -p 5433:5432 postgres`
As we can see after the `-v`, we indicate that we want the named volume to be called "odsoft_data" and will save the path `/var/lib/postgresql/data` that has the database data. After running the command everything that we save on the database will persist even if we delete the container.

But we will see on the Continuous Deployment part that we applied the volumes in a docker-compose file.

All of this information was found in [docker documentation](https://docs.docker.com/storage/).

**Generate a PDF out of a Markdown file**

To generate a PDF out of a Markdown file we used the [fntsoftware plugin](https://plugins.gradle.org/plugin/de.fntsoftware.gradle.markdown-to-pdf)
On the build.gradle file we applied the plugin in the plugins section with the folloing code line:

    id "de.fntsoftware.gradle.markdown-to-pdf" version "1.1.0"

Since `MarkdownToPdfTask` is a class, and because it is not in the Gradle namespace (it's from a 3rd party plugin) it needs to be qualified, otherwise, Gradle thinks it is a property and for that we add the following import to the top of the build.gradle file:

    import de.fntsoftware.gradle.MarkdownToPdfTask

After we are all set with the plugin we need to create the task that looks like this:

```groovy
// Generate pdf file out of a markdown file
task pdfConverter(type: MarkdownToPdfTask){
  inputFile = 'README.md' //The markdown file you want to convert
  outputFile = 'README.pdf' //the name of the pdf file you want to get
}
```

Just by telling the location of the md file and where we want the output after running the command with `./gradlew pdfConverter` the markdown file will be converted to pdf.

To the pipeline we just added this small stage:

```groovy
stage('ConvertMDtoPDF'){
    steps{
        script{
            if (isUnix()){
                sh './gradlew pdfConverter'
            }else{
                bat './gradlew pdfConverter'
            }
        }
    }
}
```

## 2.3 Code Quality and Integration Tests

1220256 João Rocha was responsible for this task

- [x] **Checkstyle**

[Checkstyle](https://checkstyle.sourceforge.io/) is a tool that verifies if the code was written follows the specified encoding rules.

Firstly, was chosen whose modules and its properties of Checkstyle should be use to check the code:

```java
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE  module PUBLIC"-//Checkstyle//DTD Checkstyle Configuration 1.3//EN"
"https://checkstyle.org/dtds/configuration_1_3.dtd">

<module  name="Checker">
	<property  name="localeLanguage"  value="en" />  <!-- Defines the language of the checks -->
	<property  name="severity"  value="warning" />  <!-- Defines the type of checks in Checker-->
	<module  name="LineLength">  <!-- Checks for long lines -->
		<property  name="max"  value="120"/>
	</module>
	<module  name="TreeWalker">
		<property  name="severity"  value="warning" />  <!-- Defines the type of checks in TreeWalker-->
	<module  name="MissingOverride"/>  <!-- Verifies that the @Override annotation is present when the @inheritDoc javadoc tag is present.-->
	<module  name="MagicNumber"/>  <!--Checks that there are no "magic numbers" where a magic number is a numeric literal that is not defined as a constant.-->
	<module  name="AvoidStarImport"/>  <!--Checks that there are no import statements that use the * notation.-->
	<module  name="IllegalImport"/>  <!-- Checks for imports from a set of illegal packages.-->
	<module  name="EmptyBlock"/>  <!-- Checks for empty blocks.-->
	<module  name="JavadocMethod"/>  <!--Checks the Javadoc of a method or constructor.-->
	<module  name="BooleanExpressionComplexity">  <!--Restricts the number of boolean operators (&&, ||, &, | and ^) in an expression.-->
		<property  name="max"  value="5"/>
	</module>
	<module  name="NoCodeInFile"/>  <!--Checks whether file contains code-->
	<module  name="LocalFinalVariableName"/>  <!--Checks that local final variable names conform to a specified pattern.-->
	<module  name="MethodLength"/>  <!-- Checks for long methods and constructors DEFAULT IS 150 -->
	<module  name="EmptyBlock"/>  <!--Checks for empty blocks-->
	<module  name="FinalLocalVariable"/>  <!--Checks that local variables that never have their values changed are declared final.-->
	<module  name="FinalParameters"/><!--Checks that parameters for methods, constructors, catch and for-each blocks are final.-->
	</module>
</module>
```

To apply the Checkstyle to this project, was added the plugin checkstyle into build.gradle, as it is shown below

```java
plugins {
id  'checkstyle'
}
```

To run Checkstyle in the pipeline was needed to add a new stage in the pipeline named "Check" which will run the 'check' task.

The 'check' task will run the checkstyleMain and checkstyleTest pre-defined tasks.

Furthermore, to publish the analysis results, was used the [Warnings Next Generation Plugin](https://www.jenkins.io/doc/pipeline/steps/warnings-ng/) to publish the issues on Jenkins, with the recordIssues.

The implementation for this is shown below.

```groovy
stage("Check"){
	steps {
		script{
			try{
				if (isUnix()){
					sh './gradlew check'
				}else{
					bat './gradlew check'
				}
			}catch (error){
				currentBuild.result =  'FAILURE'
				throw error
			}
			recordIssues(enabledForFailure:  true,  aggregatingResults:  false,
			tools:  [
				java (reportEncoding:  'UTF-8'),
				checkStyle(pattern:  '**/checkstyle/main.xml',  reportEncoding:  'UTF-8')],
			)
		}
	}
}
```

- [x] **SpotBugs**

[SpotBugs](https://spotbugs.github.io/) is a tool for static analysis to look for bugs in Java code.

To apply the SpotBugs to this project, was added the plugin com.github.spotbugs into build.gradle and some settings, as it is shown below:

```java
plugins {
id  "com.github.spotbugs"  version  "5.0.13"
}

//Task that runs spotbugs
spotbugs {
	ignoreFailures  =  true
	toolVersion  =  '4.7.3'
	reportsDir  =  file("$buildDir/reports/spotbugs")
}

spotbugsMain {
	reports {
		xml.enabled  =  true
	}
}

spotbugsTest {
	reports {
		xml.enabled  =  true
	}
}
```

When tried to run spotBugs was given an error "Failed to load class "org.slf4j.impl.StaticLoggerBinder", so to fix this was added the spotbugsSlf4j to the dependencies of the project.

```java
dependencies {
	spotbugsSlf4j  "org.slf4j:slf4j-simple:1.7.30"
}
```

To run SpotBugs in the pipeline was needed to add a tool in the stage "Check" that was created also for the Checkstyle plugin, which will run the 'check' task which uses both plugins.

From now on, the 'check' task will run the checkstyleMain, checkstyleTest, spotbugsMain and spotbugsTest.

Furthermore, to publish the analysis results, was used the [Warnings Next Generation Plugin](https://www.jenkins.io/doc/pipeline/steps/warnings-ng/) to publish the issues on Jenkins.

The implementation for this is shown below.

```groovy
stage("Check"){
	steps {
		script{
			try{
				if (isUnix()){
					sh './gradlew check'
				}else{
					bat './gradlew check'
				}
			}catch (error){
				currentBuild.result =  'FAILURE'
				throw error
			}
			recordIssues(enabledForFailure:  true,  aggregatingResults:  false,
			tools:  [
				java (reportEncoding:  'UTF-8'),
				checkStyle(pattern:  '**/checkstyle/main.xml',  reportEncoding:  'UTF-8')
				spotBugs(pattern:  '**/spotbugs/main.xml',  reportEncoding:  'UTF-8')],
            )
		}
	}
}
```

- [x] **QualityGates and Build Health**

This task is responsible to check the build health of the application and to calculate if the build of the application should continue or fail.

To achieve this task, was used the [Warnings Next Generation Plugin](https://www.jenkins.io/doc/pipeline/steps/warnings-ng/) to calculate the total of issues and to see if the app is healthier or not and if the build should continue or not.

In the jenkinsfile, was implemented the thresholds in the stage Check after the Checkstyle and Spotbugs report their issues.

```groovy
stage("Check"){
	steps {
		script{
			try{
				if (isUnix()){
					sh './gradlew check'
				}else{
					bat './gradlew check'
				}
			}catch (error){
				currentBuild.result =  'FAILURE'
				throw error
			}
			recordIssues(enabledForFailure:  true,  aggregatingResults:  false,
			tools:  [
				java (reportEncoding:  'UTF-8'),
				checkStyle(pattern:  '**/checkstyle/main.xml',  reportEncoding:  'UTF-8')
				spotBugs(pattern:  '**/spotbugs/main.xml',  reportEncoding:  'UTF-8')],
				healthy:  10,  unhealthy:  400,
				qualityGates:  [[threshold:  20,  type:  'TOTAL',  unstable:  true],  [threshold:  400,  type:  'TOTAL',  unstable:  false]]
            )
		}
	}
}
```

If the app has less or equal 10 issues, so the app is 100% healthier, if has more or equal 400 issues the app is considered 0% healthier.
If the app has less or equal 20 issues, so the app is unstable, if has more or equal 400 the build should fail.

We defined 400 as maximum threshold because the purpose of this, is that the build can continue.

- [x] **Integration Tests Coverage Build Health**

The purpose of this task is to verify the minimum build health of the integration tests coverage.

For this, was developed the following task into build.gradle.
```java
jacocoTestCoverageVerification {
	getExecutionData().setFrom("build/jacoco/integrationTest.exec")
	violationRules {
		rule {
			limit {
				minimum  =  0.2
			}
		}
	}
}
```
For the pipeline, in the integrationTest stage just was necessary to call the task which was created previously, jacocoTestCoverageVerification, as it is shown below: 
```groovy
stage("integrationTest"){
	steps{
		script{
			try{
				if (isUnix()){
					sh './gradlew integrationTest'
					sh './gradlew jacocoIntegrationReport'
					sh './gradlew jacocoTestCoverageVerification'
				}else{
					bat './gradlew integrationTest'
					bat './gradlew jacocoIntegrationReport'
					bat './gradlew jacocoTestCoverageVerification'
				}
				publishHTML([allowMissing:  false,  alwaysLinkToLastBuild:  false,  keepAll:  false,  reportDir:  'build/htmlReports/junitReports/integration',  reportFiles:  'index.html',  reportName:  'IntegrationTests Report',  reportTitles:  '',  useWrapperFileDirectly:  true])
				publishHTML([allowMissing:  false,  alwaysLinkToLastBuild:  false,  keepAll:  false,  reportDir:  'build/reports/jacoco/jacocoIntegrationReport/html',  reportFiles:  'index.html',  reportName:  'IntegrationTests Coverage Report',  reportTitles:  '',  useWrapperFileDirectly:  true])
			}catch (error){
				currentBuild.result =  'FAILURE'
				throw error
			}
		}
	}
}
```
If the Integration Test Coverage has less than 0.2 of coverage than the build should fail, so for this we chose 0.2 as a minimum of coverage so that the build can continue.

## 2.6 Continuous Deployment

The student, Gonçalo Pinho-1220257 was the one in charge of the documentation and database.

\*\*

On this part of the assignment we were supposed to simulate a production environment with docker and deploy the project there.
The persistence layer that we talked about in the point 2.2, is going to be applied on the production environment because it doesn't make sense to persist data on the staging environment.

Since docker has the ability to define and run multi-container docker applications we also used a docker-compose.yml file to define the two services that will run at the same time. It's the same logic as the staging environment but with the difference that it persists the data of the database.

```yml
version: '3.1'
services:
  app:
    container_name: vaadin-crm
    image: odsoft-image
    build: ./
    ports:
      - "8082:8080"
  environment:
      - DB_HOST=postgresqlcrm
      - DB_PORT=5432
    depends_on:
      - postgresqlcrm
  postgresqlcrm:
    container_name: postgresqlcrm
    image: postgres
    ports:
      - "5433:5432"
  environment:
      - POSTGRES_PASSWORD=12345
      - POSTGRES_USER=postgres
      - POSTGRES_DB=odsoft
    volumes:
      - "odsoft_data:/var/lib/postgresql/data"
volumes:
  odsoft_data:
```

As we can see we have two services the`app`and the `postgresqlcrm` the app will use our previously built docker image, and we will be able to access the project though the port 8082. Since we need to change databases we configured the application.properties to be able to connect to the database via environment variables, as we can see we have two of them, the DB_HOST and DB_PORT. There's something we need to be careful while creating multi-containers that one depends on another like on this case.When you want to access something that is inside other container you need to use the name of that container as the IP like we can see on the previous docker-compose.

All of this information was found in [docker compose documentation](https://docs.docker.com/compose/compose-file/)

To run this file on the pipeline we used the following stage:

```groovy
stage("DeployProd"){
    steps {
        script{
            try{
                if (isUnix()){
                    sh 'docker-compose -f docker-compose-staging.yml down'
                    sh 'docker-compose up -d'
                }else{
                    bat 'docker-compose -f docker-compose-staging.yml down'
                    bat 'docker-compose up -d'
                }
            }catch (error){
                currentBuild.result = 'FAILURE'
  throw error
            }
        }
    }
}
```

As we can see on the code snipped, first, we shutdown the docker-compose-staging and then we start the docker-compose that has the production environment.

After the environment is up, we perform a smoke test:

```groovy
stage('SmoketestProd'){
    steps{
        script{
            try{
                if (isUnix()){
                    httpCode = sh( script: "curl -s -o /dev/null -w '%{http_code}' $url/login", returnStdout: true ).trim()
                    echo httpCode
                }else{
                    httpCode = bat( script: "curl -s -o ./response -w %%{http_code} $url/login", returnStdout: true).trim()
                    httpCode = httpCode.readLines().drop(1).join(" ")//windows returns full command plus the response, but the response is at a new line so we can drop the first line and remove spaces and we get only the http code
                }
                //checking if the http code was ok(200) or found(302)
  if (httpCode == "200" || httpCode == "302"){
                    echo 'The application is responding!'
                }else{
                    currentBuild.result = 'FAILURE'
  error('The application is not responding...')
                }
            }catch(error){
                currentBuild.result = 'FAILURE'
  throw error
            }
        }
    }
}
```

- [x] **End To End testing**

End-to-end (E2E) tests are a type of automated testing that involves simulating the actions of a real user interacting with your application. They are designed to test the entire workflow of your application, from start to finish, to ensure that it is functioning correctly and as intended.

Selenium is a popular open-source tool that is often used for creating E2E tests. It allows you to automate the testing of web applications by simulating the actions of a user interacting with a web browser. To execute this test, we need to add these dependencies to the `build.gradle`file:
```groovy
//selenium  
testImplementation 'org.seleniumhq.selenium:selenium-java:4.7.0'  
testImplementation("io.github.bonigarcia:webdrivermanager:5.3.1")
```
We also need to build the following script that authenticates as an administrator and adds a new product to the database:
```groovy
public class ProductE2E {  
  
    @Test  
  public void createProduct() throws InterruptedException {  
        if(SystemUtils.IS_OS_WINDOWS){  
            System.setProperty("webdriver.chrome.driver", "chromedriver.exe");  
		}  
        else{  
            System.setProperty("webdriver.chrome.driver", "chromedriver");  
		}  
  
        WebDriver driver = new ChromeDriver();  
		driver.get("http://localhost:8082/login");  
		//Wait until the page is ready  
		driver.manage().window().maximize();  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(5)).until(titleIs("Login | Vaadin CRM"));  
  
		//Login page elements  
		WebElement usernameField = driver.findElement(By.cssSelector("#input-vaadin-text-field-6"));  
		WebElement passwordField = driver.findElement(By.cssSelector("#input-vaadin-password-field-7"));  
		WebElement loginButton = driver.findElement(By.cssSelector("vaadin-button[role='button']"));  
  
		//Login action  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(usernameField));  
		usernameField.click();  
		usernameField.sendKeys("admin");  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(passwordField));  
		passwordField.click();  
		passwordField.sendKeys("userpass");  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(loginButton));  
		loginButton.click();  
		  
		//Contacts page  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(5) .until(titleIs("Contacts | Vaadin CRM"));  
		  
		//Contacts page elements  
		WebElement productsButton = driver.findElement(By.cssSelector("a[href='product']"));  
		  
		//Go to products  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(productsButton));  
		productsButton.click();  
  
		//Products page  
		String dummyProductCategory = "Soda";  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(5)).until(titleIs("Product Categories | Vaadin CRM"));  
		  
		//Products categories page elements  
		WebElement addProd = driver.findElement(By.cssSelector("vaadin-horizontal-layout[class='toolbar'] vaadin-button[role='button']"));  
  
		//Open form  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(addProd));  
		addProd.click();  
		  
		WebElement prodForm = driver.findElement(By.cssSelector("#ROOT-2521314 > vaadin-app-layout > vaadin-vertical-layout.list-view > div > vaadin-form-layout"));  
		  
		new WebDriverWait(driver, ofSeconds(10)).until(visibilityOf(prodForm));  
		  
		//Finds textfield and populates with dummy data  
		WebElement textfield = driver.findElement(By.cssSelector("#input-vaadin-text-field-11"));  
		textfield.click();  
		textfield.sendKeys(dummyProductCategory);  
		textfield.sendKeys(Keys.ENTER);  
		//Find the last product inserted in the grid  
		String xPathStart = "//vaadin-grid-cell-content[contains(.,'";  
		String xPathEnd = "')]";  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(1)).until(visibilityOfElementLocated(By.xpath(xPathStart + dummyProductCategory + xPathEnd)));  
  
		//Gets the cells in the table for the newly added product  
		WebElement lastDescriptionCell = driver.findElement(By.xpath(xPathStart + dummyProductCategory + xPathEnd));  
		  
		Assertions.assertEquals(dummyProductCategory, lastDescriptionCell.getText());  
  }  
}
```
We also did the same test for the suppliers:

```groovy
public class SupplierE2E {  
  
    @Test  
  public void createSupplier() throws InterruptedException {  
  
        if(SystemUtils.IS_OS_WINDOWS){  
            System.setProperty("webdriver.chrome.driver", "chromedriver.exe");  
		}  
	     else{  
            System.setProperty("webdriver.chrome.driver", "chromedriver");  
		}  
  
        WebDriver driver = new ChromeDriver();  
		driver.get("http://localhost:8082/login");  
		//Wait until the page is ready  
		driver.manage().window().maximize();  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(5)).until(titleIs("Login | Vaadin CRM"));  
  
		//Login page elements  
		WebElement usernameField = driver.findElement(By.cssSelector("#input-vaadin-text-field-6"));  
		WebElement passwordField = driver.findElement(By.cssSelector("#input-vaadin-password-field-7"));  
		WebElement loginButton = driver.findElement(By.cssSelector("vaadin-button[role='button']"));  
		  
		//Login action  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(usernameField));  
		usernameField.click();  
		usernameField.sendKeys("admin");  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(passwordField));  
		passwordField.click();  
		passwordField.sendKeys("userpass");  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(loginButton));  
		loginButton.click();  
  
		//Contacts page  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(5)).until(titleIs("Contacts | Vaadin CRM"));  
		  
		//Contacts page elements  
		WebElement suppliersButton = driver.findElement(By.cssSelector("a[href='suppliers']"));  
		  
		//Go to suppliers  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(suppliersButton));  
		suppliersButton.click();  
  
		//Suppliers page  
		String dummySupplierName = "Nasa";  
		String dummySupplierAddress = "Somewhere up there";  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(5)).until(titleIs("Suppliers | Vaadin CRM"));  
  
		//Suppliers page elements  
		WebElement addSupplier = driver.findElement(By.cssSelector("#ROOT-2521314 > vaadin-app-layout > vaadin-vertical-layout.list-view > vaadin-horizontal-layout > vaadin-button"));  
  
		//Open form  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(10)).until(ExpectedConditions.elementToBeClickable(addSupplier));  
		addSupplier.click();  
		  
		WebElement supplierForm = driver.findElement(By.cssSelector("#ROOT-2521314 > vaadin-app-layout > vaadin-vertical-layout.list-view > div > vaadin-form-layout"));  
		  
		new WebDriverWait(driver, ofSeconds(10)).until(visibilityOf(supplierForm));  
		  
		//Populating with dummy data  
		WebElement textfieldName = driver.findElement(By.cssSelector("#input-vaadin-text-field-11"));  
		textfieldName.click();  
		textfieldName.sendKeys(dummySupplierName);  
		  
		WebElement textfieldAddress = driver.findElement(By.cssSelector("#input-vaadin-text-field-15"));  
		textfieldAddress.click();  
		textfieldAddress.sendKeys(dummySupplierAddress);  
  
  
		WebElement multiSelect = driver.findElement(By.cssSelector("#input-vaadin-multi-select-combo-box-20"));  
		multiSelect.click();  
		  
		WebElement option1 = driver.findElement(By.cssSelector("#vaadin-multi-select-combo-box-item-0"));  
		WebElement option2 = driver.findElement(By.cssSelector("#vaadin-multi-select-combo-box-item-1"));  
		option1.click();  
		option2.click();  
		  
		multiSelect.sendKeys(Keys.ESCAPE);  
		multiSelect.sendKeys(Keys.ENTER);  
		  
		//Find the last supplier inserted in the grid  
		String xPathStart = "//vaadin-grid-cell-content[contains(.,'";  
		String xPathEnd = "')]";  
		new WebDriverWait(driver, ofSeconds(30), ofSeconds(1)).until(visibilityOfElementLocated(By.xpath(xPathStart + dummySupplierName + xPathEnd)));  
  
		//Gets the cells in the table for the newly added supplier  
		WebElement lastNameCell = driver.findElement(By.xpath(xPathStart + dummySupplierName + xPathEnd));  
		System.out.println(lastNameCell.getText());  
		Assertions.assertEquals(dummySupplierName, lastNameCell.getText());  
	  }  
}
```
These tests can be executed with the following command `./gradlew endToEnd` that will perform this task:
```groovy
//End2End task matching the file name by convention and generating a report  
task endToEnd(type: Test) {  
  useJUnitPlatform()  
  reports.html.setDestination(file("build/htmlReports/selenium/end2end")) //setting output directory for the test result  
  filter{  
  includeTestsMatching "*E2E" // filter by name convention  
  }  
}
```
